{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from sklearn.metrics import accuracy_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of files for the test data\n",
    "testfile = open(\"test.txt\", \"r\")\n",
    "testlist = testfile.readlines()\n",
    "testlist = [file[:-1]+\".flac\" for file in testlist]\n",
    "\n",
    "# Getting a list of files for the training data\n",
    "trainfile = open(\"train.txt\", \"r\")\n",
    "trainlist = trainfile.readlines()\n",
    "trainlist = [file[:-1]+\".flac\" for file in trainlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the file locations and their labels\n",
    "testnames = []\n",
    "trainnames = []\n",
    "trainlabels = []\n",
    "testlabels = []\n",
    "for filename in glob.iglob('./LibriSpeech/dev-clean/*/**', recursive=True):\n",
    "    if os.path.isfile(filename) and '.flac' in filename: # filter dirs\n",
    "        name = filename\n",
    "        if name.split('\\\\')[3] in trainlist:\n",
    "            trainnames.append(name)\n",
    "            label = name.split('\\\\')[1]\n",
    "            trainlabels.append(label)\n",
    "        elif name.split('\\\\')[3] in testlist:\n",
    "            testnames.append(name)\n",
    "            label = name.split('\\\\')[1]\n",
    "            testlabels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the Labels as One-Hot, printing the number of classes\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(trainlabels)\n",
    "test_labels = label_encoder.fit_transform(testlabels)\n",
    "n_classes = len(np.unique(train_labels))\n",
    "print(\"nclasses:\", n_classes)\n",
    "binarize = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "train_labels = binarize.fit_transform(train_labels)\n",
    "test_labels = binarize.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the files and preprocessing (getting and slicing MFCCs)\n",
    "def getfiles(names, labels):\n",
    "    output_data = []\n",
    "    output_labels = []\n",
    "    for i, n in enumerate(names):\n",
    "        data, Fs = sf.read(n)\n",
    "        mfcc = librosa.feature.mfcc(data, Fs, n_mfcc=40)[:,:100]\n",
    "        if mfcc.shape[1]==100:\n",
    "            dat = [mfcc, mfcc, mfcc]\n",
    "            output_data.append(dat)\n",
    "            output_labels.append(labels[i])\n",
    "    return output_data, output_labels\n",
    "\n",
    "x_trainval, y_trainval = getfiles(trainnames, train_labels)\n",
    "x_test, y_test = getfiles(testnames, test_labels)\n",
    "x_train, x_val = train_test_split(x_trainval, train_size = 0.9)\n",
    "y_train, y_val = train_test_split(y_trainval, train_size = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offloading the dataset to the GPU\n",
    "x_train = torch.tensor(x_train).permute(0, 1, 3, 2).to(device)\n",
    "x_test = torch.tensor(x_test).permute(0, 1, 3, 2).to(device)\n",
    "x_val = torch.tensor(x_val).permute(0, 1, 3, 2).to(device)\n",
    "y_train = torch.DoubleTensor(y_train).to(device)\n",
    "y_test = torch.DoubleTensor(y_test).to(device)\n",
    "y_val = torch.DoubleTensor(y_val).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataloader for the training\n",
    "trainingdat = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainingdat, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "# Taking the existing ResNet-18 architecture\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
    "model.double()\n",
    "model = nn.Sequential(*list(model.children())[:-2]) #Taking out averaging and FC layers\n",
    "\n",
    "n_k = 4 #as selected in the paper\n",
    "n_c = 16 \n",
    "beta = .0001\n",
    "\n",
    "# Creating the full network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.ResNet = model\n",
    "        self.linear1 = nn.Linear(1024, n_c)\n",
    "        self.linear2 = nn.Linear(n_c, n_k)\n",
    "        self.bn1 = nn.BatchNorm1d(4)\n",
    "        self.pool_time = nn.AdaptiveAvgPool2d((1, 1024))\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # RESNET LAYERS\n",
    "        x1 = self.ResNet(x)\n",
    "        \n",
    "        # CONVERT X FOR SELF-ATTENTION\n",
    "        x = x1.permute(0, 2, 1, 3)\n",
    "        x = nn.Flatten(2,3)(x)\n",
    "        \n",
    "        # SELF-ATTENTION\n",
    "        a = self.linear1(x)\n",
    "        A = nn.Softmax(1)(self.linear2(nn.Tanh()(a)))\n",
    "        A = A.permute(0,2,1)\n",
    "        x = torch.matmul(A, x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # FINAL LAYERS\n",
    "        x = self.pool_time(x)\n",
    "        x = nn.Flatten(1,2)(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.Softmax(1)(x)        \n",
    "        return x, A\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "net.double()\n",
    "\n",
    "# Testing the network with toy data to see if there are errors\n",
    "tensor1 = torch.randn(2, 3, 100, 40).double()\n",
    "output, _ = net(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the network structure\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrop_loss_function(prds, actual):\n",
    "    crossentropy = -torch.sum(torch.sum(actual * torch.log(prds), dim=1))\n",
    "    return crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_loss_function(A):\n",
    "    Asum = torch.sum(A, dim=0)\n",
    "    mat = torch.mm(Asum, Asum.T)\n",
    "    mat = mat - torch.eye(n_k).double()\n",
    "    l = torch.norm(mat, p='fro')**2\n",
    "    return(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward step\n",
    "        outputs, A = net(inputs)\n",
    "        \n",
    "        # CALCULATING THE LOSS\n",
    "        entrop_loss = entrop_loss_function(outputs, labels)\n",
    "        p_loss = p_loss_function(A)\n",
    "        loss = entrop_loss + beta * p_loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    if ep % 5 == 0:\n",
    "        torch.save(net.state_dict(), \"net_weights_%d.mdl\" % (ep))\n",
    "\n",
    "        preds, _ = net(x_val)\n",
    "        preds = (preds == torch.max(preds, dim=1, keepdim=True)[0]).type(torch.int)\n",
    "        print(\"Validation loss:\", accuracy_score(y_val, preds))\n",
    "        print(\"\")\n",
    "\n",
    "        preds, _ = net(x_train)\n",
    "        preds = (preds == torch.max(preds, dim=1, keepdim=True)[0]).type(torch.int)\n",
    "        print(\"Training loss:\", accuracy_score(y_train, preds))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the accuracy on the testing data once training is finished\n",
    "preds, _ = net(x_test)\n",
    "preds = (preds == torch.max(preds, dim=1, keepdim=True)[0]).type(torch.int)\n",
    "print(\"Training loss:\", accuracy_score(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ASR]",
   "language": "python",
   "name": "conda-env-ASR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
