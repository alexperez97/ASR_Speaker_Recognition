{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:ASR]",
      "language": "python",
      "name": "conda-env-ASR-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqwDy7jDQYAH"
      },
      "source": [
        "# sample execution (requires torchvision)\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import glob, os\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xhn28K5QpJC"
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append(\"/content/drive/My Drive/Master/ASR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYMEj9YCQYAH"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc_S9YbKQYAH"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3e1Qu26QYAH"
      },
      "source": [
        "testfile = open(\"/content/drive/My Drive/Master/ASR/test.txt\", \"r\")\n",
        "testlist = testfile.readlines()\n",
        "testlist = [file[:-1]+\".flac\" for file in testlist]\n",
        "\n",
        "trainfile = open(\"/content/drive/My Drive/Master/ASR/train.txt\", \"r\")\n",
        "trainlist = trainfile.readlines()\n",
        "trainlist = [file[:-1]+\".flac\" for file in trainlist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4qT1fsYQYAH"
      },
      "source": [
        "testnames = []\n",
        "trainnames = []\n",
        "trainlabels = []\n",
        "testlabels = []\n",
        "for filename in glob.iglob('/content/drive/My Drive/Master/ASR/LibriSpeech/dev-clean/*/**', recursive=True):\n",
        "    if os.path.isfile(filename) and '.flac' in filename: # filter dirs\n",
        "        name = filename\n",
        "        if name.split('/')[-1] in trainlist:\n",
        "            trainnames.append(name)\n",
        "            label = name.split('/')[-3]\n",
        "            trainlabels.append(label)\n",
        "        elif name.split('/')[-1] in testlist:\n",
        "            testnames.append(name)\n",
        "            label = name.split('/')[-3]\n",
        "            testlabels.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lscOwkqQYAH"
      },
      "source": [
        "# Encoding the Labels as One-Hot\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(trainlabels)\n",
        "test_labels = label_encoder.fit_transform(testlabels)\n",
        "n_classes = len(np.unique(train_labels))\n",
        "print(\"nclasses:\", n_classes)\n",
        "binarize = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
        "train_labels = binarize.fit_transform(train_labels)\n",
        "test_labels = binarize.fit_transform(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QGOh0t1QYAH"
      },
      "source": [
        "def getfiles(names, labels):\n",
        "    output_data = []\n",
        "    output_labels = []\n",
        "    for i, n in enumerate(names):\n",
        "        data, Fs = sf.read(n)\n",
        "        mfcc = librosa.feature.mfcc(data, Fs, n_mfcc=40)[:,:100]\n",
        "        if mfcc.shape[1]==100:\n",
        "            dat = [mfcc, mfcc, mfcc]\n",
        "            output_data.append(dat)\n",
        "            output_labels.append(labels[i])\n",
        "    return output_data, output_labels\n",
        "\n",
        "x_trainval, y_trainval = getfiles(trainnames, train_labels)\n",
        "x_test, y_test = getfiles(testnames, test_labels)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, train_size = 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQZui-P8QYAH"
      },
      "source": [
        "x_train = torch.tensor(x_train).permute(0, 1, 3, 2).to(device)\n",
        "x_test = torch.tensor(x_test).permute(0, 1, 3, 2).to(device)\n",
        "x_val = torch.tensor(x_val).permute(0, 1, 3, 2).to(device)\n",
        "y_train = torch.DoubleTensor(y_train).to(device)\n",
        "y_test = torch.DoubleTensor(y_test).to(device)\n",
        "y_val = torch.DoubleTensor(y_val).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "255H1NiBQYAH"
      },
      "source": [
        "trainingdat = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainingdat, batch_size=5, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRC-AsswQYAH"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyaudbfzQYAH"
      },
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
        "model.double()\n",
        "model = nn.Sequential(*list(model.children())[:-2]) #Taking out averaging and FC layers\n",
        "#print(model)\n",
        "\n",
        "n_k = 4\n",
        "n_c = 16 \n",
        "beta = .0001\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.ResNet = model\n",
        "        self.linear1 = nn.Linear(1024, n_c)\n",
        "        self.linear2 = nn.Linear(n_c, n_k)\n",
        "        self.bn1 = nn.BatchNorm1d(4)\n",
        "        self.pool_time = nn.AdaptiveAvgPool2d((1, 1024))\n",
        "        self.fc1 = nn.Linear(1024, 256)\n",
        "        self.fc2 = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # RESNET LAYERS\n",
        "        x1 = self.ResNet(x)\n",
        "        \n",
        "        # CONVERT X FOR SELF-ATTENTION\n",
        "        x = x1.permute(0, 2, 1, 3) #For some reason the dimensions seemed to be in different order?\n",
        "        x = nn.Flatten(2,3)(x)\n",
        "        \n",
        "        # SELF-ATTENTION\n",
        "        a = self.linear1(x)\n",
        "        A = nn.Softmax(1)(self.linear2(nn.Tanh()(a)))\n",
        "        A = A.permute(0,2,1)\n",
        "        x = torch.matmul(A, x)\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        # FINAL LAYERS\n",
        "        x = self.pool_time(x)\n",
        "        x = nn.Flatten(1,2)(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = nn.Softmax(1)(x)        \n",
        "        return x, A\n",
        "\n",
        "net = Net()\n",
        "net.to(device)\n",
        "net.double()\n",
        "#print(net)\n",
        "tensor1 = torch.randn(2, 3, 100, 40).double().to(device)\n",
        "output, _ = net(tensor1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlJ31OP-QYAI"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQbwzpDsQYAI"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm8B_7SyQYAI"
      },
      "source": [
        "def entrop_loss_function(prds, actual):\n",
        "    crossentropy = -torch.sum(torch.sum(actual * torch.log(prds), dim=1))\n",
        "    return crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aan4vAjDQYAI"
      },
      "source": [
        "def p_loss_function(A):\n",
        "    Asum = torch.sum(A, dim=0)\n",
        "    mat = torch.mm(Asum, Asum.T)\n",
        "    mat = mat - torch.eye(n_k).double().to(device)\n",
        "    l = torch.norm(mat, p='fro')**2\n",
        "    return(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qRDNJIcQYAI"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlQsC5yHQYAI"
      },
      "source": [
        "#net.load_state_dict(torch.load())\n",
        "import time\n",
        "start = time.time()\n",
        "acc_train = []\n",
        "acc_val = []\n",
        "eps = []\n",
        "\n",
        "for epoch in range(26):  # loop over the dataset multiple times\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      torch.save(net.state_dict(), \"/content/drive/My Drive/Master/ASR/weights/2net_weights_%d.mdl\" % (epoch))\n",
        "\n",
        "      print(\"Epoch:\", epoch)\n",
        "      eps.append(epoch)\n",
        "          \n",
        "      preds, _ = net(x_train)\n",
        "      preds = (preds == torch.max(preds, dim=1, keepdim=True)[0]).type(torch.int).to(device)\n",
        "      trainingacc = accuracy_score(y_train.cpu(), preds.cpu())\n",
        "      print(\"Training accuracy:\", trainingacc)\n",
        "      acc_train.append(trainingacc)\n",
        "\n",
        "      preds1org, _ = net(x_val)\n",
        "      preds1 = (preds1org == torch.max(preds1org, dim=1, keepdim=True)[0]).type(torch.int).to(device)\n",
        "      validacc = accuracy_score(y_val.cpu(), preds1.cpu())\n",
        "      print(\"Validation accuracy:\", validacc)\n",
        "      acc_val.append(validacc)\n",
        "\n",
        "      print(\"loss\", running_loss)\n",
        "\n",
        "      print(\"\")\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward step\n",
        "        outputs, A = net(inputs)\n",
        "        \n",
        "        # CALCULATING THE LOSS\n",
        "        entrop_loss = entrop_loss_function(outputs, labels)\n",
        "        p_loss = p_loss_function(A)\n",
        "        loss = entrop_loss + beta * p_loss\n",
        "        #print(loss)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "    \n",
        "print('Finished Training')\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRuCMSKZQYAI"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwK7C6zgQYAI"
      },
      "source": [
        "preds, _ = net(x_test)\n",
        "preds = (preds == torch.max(preds, dim=1, keepdim=True)[0]).type(torch.int)\n",
        "print(\"Training loss:\", accuracy_score(y_test.cpu(), preds.cpu()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qExA-1ojYF8b"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (13, 5)\n",
        "plt.rc('font', size=14)\n",
        "sns.lineplot(x=eps, y=acc_train, label=\"Train Accuracy\")\n",
        "sns.lineplot(x=eps, y=acc_val, label=\"Validation Accuracy\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy Value\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPbqCHXZj8jN"
      },
      "source": [
        "preds_values = []\n",
        "for pred in preds:\n",
        "    preds_values.append(pred.argmax().item())\n",
        "\n",
        "test_values = []\n",
        "for y in y_test:\n",
        "    test_values.append(y.argmax().item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXcMPHH-i0wX"
      },
      "source": [
        "data = {'y_Actual': test_values, 'y_Predicted': preds_values}\n",
        "df = pd.DataFrame(data, columns=['y_Actual', 'y_Predicted'])\n",
        "df.rename(columns={\"0.0\": \"Unseen\", \"1.0\": \"Seen\"})\n",
        "confusion = pd.crosstab(df['y_Actual'],\n",
        "                        df['y_Predicted'],\n",
        "                        rownames=['Actual'],\n",
        "                        colnames=['Predicted'])\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.rc('font', size=14)\n",
        "sns.heatmap(confusion,fmt='g', square=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}